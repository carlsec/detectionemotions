{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wb2D1oD9HQeZ"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import pathlib\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 23 03:03:13 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   35C    P0     2W /  38W |    320MiB /  1999MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1298      G   /usr/lib/xorg/Xorg                135MiB |\n",
      "|    0   N/A  N/A      1531      G   /usr/bin/gnome-shell               55MiB |\n",
      "|    0   N/A  N/A      2426      G   ...gAAAAAAAAA --shared-files        7MiB |\n",
      "|    0   N/A  N/A     30624      G   ...AAAAAAAAA= --shared-files      111MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ac8FV849dv2U"
   },
   "outputs": [],
   "source": [
    "EMOTIONS = ['angry', 'scared', 'happy', 'sad',\n",
    "          'surprised','neutral']\n",
    "\n",
    "COLORS = {'angry': (0, 0, 255),\n",
    "    'scared': (0, 128, 255),\n",
    "    'happy': (0, 255, 255),\n",
    "    'sad': (255, 0, 0),\n",
    "    'surprised': (178, 255, 102),\n",
    "    'neutral': (160, 160, 160)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = ['bravo', 'sarcastisco', 'feliz', 'triste',\n",
    "          'surpreso','neutro']\n",
    "\n",
    "COLORS = {'bravo': (0, 0, 255),\n",
    "    'sarcastisco': (0, 128, 255),\n",
    "    'feliz': (0, 255, 255),\n",
    "    'triste': (255, 0, 0),\n",
    "    'surpreso': (178, 255, 102),\n",
    "    'neutro': (160, 160, 160)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3IM-LUq5dv4o"
   },
   "outputs": [],
   "source": [
    "def build_network(input_shape, classes):\n",
    "    input = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=(3,3), padding='same', kernel_initializer='he_normal')(input)\n",
    "    x = ELU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(filters=32, kernel_size=(3,3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = ELU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = ELU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = ELU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3,3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = ELU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3,3), padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = ELU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=64, kernel_initializer='he_normal')(x)\n",
    "    x = ELU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=64, kernel_initializer='he_normal')(x)\n",
    "    x = ELU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=classes, kernel_initializer='he_normal')(x)\n",
    "    output = Softmax()(x)\n",
    "\n",
    "    return Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4U8lKPo4dv7H"
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, classes):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    val_images = []\n",
    "    val_labels = []\n",
    "\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            label = int(line['emotion'])\n",
    "\n",
    "        if label <= 1:\n",
    "            label = 0\n",
    "\n",
    "        if label > 0:\n",
    "            label -= 1\n",
    "\n",
    "        image = np.array(line['pixels'].split(' '), dtype='uint8')\n",
    "        image = image.reshape((48, 48))\n",
    "        image = img_to_array(image)\n",
    "\n",
    "        if line['Usage'] == 'Training':\n",
    "            train_images.append(image)\n",
    "            train_labels.append(label)\n",
    "        elif line['Usage'] == 'PrivateTest':\n",
    "            val_images.append(image)\n",
    "            val_labels.append(label)\n",
    "        else:\n",
    "            test_images.append(image)\n",
    "            test_labels.append(label)\n",
    "\n",
    "    train_images = np.array(train_images)\n",
    "    val_images = np.array(val_images)\n",
    "    test_images = np.array(test_images)\n",
    "  \n",
    "    train_labels = to_categorical(np.array(train_labels), classes)\n",
    "    val_labels = to_categorical(np.array(val_labels), classes)\n",
    "    test_labels = to_categorical(np.array(test_labels), classes)\n",
    "\n",
    "    return (train_images, train_labels), \\\n",
    "         (val_images, val_labels), \\\n",
    "         (test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JUqLLqusmEVQ"
   },
   "outputs": [],
   "source": [
    "def rectangle_area(r):\n",
    "    return (r[2] - r[0]) * (r[3] - r[1])\n",
    "\n",
    "def plot_emotion(emotions_plot, emotion, probability, index):\n",
    "    w = int(probability * emotions_plot.shape[1])\n",
    "    cv2.rectangle(emotions_plot,\n",
    "                (5, (index * 35) + 5),\n",
    "                (w, (index * 35) + 35),\n",
    "                color=COLORS[emotion],\n",
    "                thickness=-1)\n",
    "  \n",
    "    white = (255,255,255)\n",
    "    text = f'{emotion}: {probability * 100:.2f}%'\n",
    "    cv2.putText(emotions_plot,\n",
    "              text,\n",
    "              (10, (index * 35) + 23),\n",
    "              fontFace = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "              fontScale=0.45,\n",
    "              color=white,\n",
    "              thickness=2)\n",
    "  \n",
    "    return emotions_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ekzq1vx0mEYK"
   },
   "outputs": [],
   "source": [
    "def plot_face(image, emotion, detection):\n",
    "    frame_x, frame_y, frame_width, frame_height = detection\n",
    "    cv2.rectangle(image,\n",
    "                  (frame_x, frame_y),\n",
    "                  (frame_x + frame_width,\n",
    "                   frame_y + frame_height),\n",
    "                  color=COLORS[emotion],\n",
    "                  thickness=2)\n",
    "    cv2.putText(image,\n",
    "                emotion,\n",
    "                (frame_x, frame_y - 10),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=0.45,\n",
    "                color=COLORS[emotion],\n",
    "                thickness=2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_mHrB8JumEbA"
   },
   "outputs": [],
   "source": [
    "def predict_emotion(model, roi):\n",
    "    roi = cv2.resize(roi, (48,48))\n",
    "    roi = roi.astype('float') / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    predictions = model.predict(roi)[0]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "579Yo2Xiy-K8"
   },
   "outputs": [],
   "source": [
    "checkpoints = sorted(list(glob.glob('./*.h5')), reverse=True)\n",
    "if len(checkpoints) > 0:\n",
    "    model = load_model(checkpoints[0])\n",
    "else:\n",
    "    input_path = str('/content/drive/MyDrive/fer2013.csv')\n",
    "    classes = len(EMOTIONS)\n",
    "\n",
    "    (train_images, train_labels), \\\n",
    "    (val_images, val_labels), \\\n",
    "    (test_images, test_labels) = load_dataset(input_path, classes)\n",
    "\n",
    "    model = build_network((48,48,1), classes)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=Adam(learning_rate=0.003),\n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "    checkpoint_pattern = ('model-ep{epoch:03d} - loss{loss:.3f}'\n",
    "                        '-val_loss{val_loss:.3f}.h5')\n",
    "  \n",
    "    checkpoint = ModelCheckpoint(checkpoint_pattern,\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True,\n",
    "                               mode='min')\n",
    "  \n",
    "    BATCH_SIZE = 128\n",
    "\n",
    "    train_augmenter = ImageDataGenerator(rotation_range=10,\n",
    "                                       zoom_range=0.1,\n",
    "                                       horizontal_flip=True,\n",
    "                                       rescale=1. / 255.,\n",
    "                                       fill_mode='nearest')\n",
    "    train_gen = train_augmenter.flow(train_images,\n",
    "                                   train_labels,\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "    train_steps = len(train_images) // BATCH_SIZE\n",
    "\n",
    "    val_augmenter = ImageDataGenerator(rescale=1. / 255.)\n",
    "    val_gen = val_augmenter.flow(val_images,\n",
    "                                 val_labels,\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "  \n",
    "    EPOCHS = 300\n",
    "    model.fit(train_gen,\n",
    "            steps_per_epoch=train_steps,\n",
    "            validation_data=val_gen,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            callbacks=[checkpoint])\n",
    "  \n",
    "    test_augmenter = ImageDataGenerator(rescale=1. / 255.)\n",
    "    test_gen = test_augmenter.flow(test_images,\n",
    "                                   test_labels,\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "    test_steps = len(test_images) // BATCH_SIZE\n",
    "    _, accuracy = model.evaluate(test_gen, steps=test_steps)\n",
    "\n",
    "    print(f'Accuracy: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SDSIvs8tI2XQ"
   },
   "outputs": [],
   "source": [
    "video_path = 'emotions.mp4'\n",
    "camera = cv2.VideoCapture(0)  # Pass 0 to use webcam\n",
    "\n",
    "cascade_file = 'haarcascade_frontalface_default.xml'\n",
    "det = cv2.CascadeClassifier(cascade_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hlDwbiB67dQj"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e7384243e6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mframe_exists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mframe_exists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    frame_exists, frame = camera.read()\n",
    "\n",
    "    if not frame_exists:\n",
    "        break\n",
    "  \n",
    "    frame = imutils.resize(frame, width=380)\n",
    "    emotions_plot = np.zeros_like(frame, dtype='uint8')\n",
    "    copy = frame.copy()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    detections = det.detectMultiScale(gray,\n",
    "                          scaleFactor=1.1,\n",
    "                          minNeighbors=5,\n",
    "                          minSize=(35, 35),\n",
    "                          flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "  \n",
    "    if len(detections) > 0:\n",
    "        detections = sorted(detections, key = rectangle_area)\n",
    "        best_detection = detections[-1]\n",
    "\n",
    "        (frame_x, frame_y,\n",
    "             frame_width, frame_height) = best_detection\n",
    "\n",
    "        roi = gray[frame_y:frame_y + frame_height,\n",
    "                   frame_x:frame_x + frame_width]\n",
    "        predictions = predict_emotion(model, roi)\n",
    "\n",
    "        label = EMOTIONS[predictions.argmax()]\n",
    "\n",
    "        for i, (emotion, probability) in \\\n",
    "                enumerate(zip(EMOTIONS, predictions)):\n",
    "            emotions_plot = plot_emotion(emotions_plot,\n",
    "                                    emotion,\n",
    "                                    probability, i)\n",
    "        \n",
    "        close = plot_face(copy, label, best_detection)\n",
    "  \n",
    "    cv2.imshow('Face & Emotions', np.hstack([copy, emotions_plot]))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWBHK_4Yy-RY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "S3j5inPVdv95"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "EmotionDetector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
